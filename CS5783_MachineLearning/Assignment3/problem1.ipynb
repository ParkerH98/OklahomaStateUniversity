{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important Notes\n",
    "\n",
    "- Each model has one that is tuned by Keras and one that I plugged the tuned values into. The Keras-Tuner models will be in the tuner.ipynb file while my tuned submission models will be in this file.\n",
    "- Keras doesn't natively support batch size optimizations so I had to optimize them with for loops. Therefore, I wasn't able to put batch size in the combination of things Keras tested. This can cause less than optimal results.\n",
    "- Each model trained to a 98% validation accuracy after 5 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension.\n",
    "# %load_ext tensorboard\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.layers as Layer\n",
    "\n",
    "import tensorboard\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import keras_tuner as kt\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Portion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset and normalize\n",
    "(train_images, train_labels), (test_images, test_labels) = keras.datasets.mnist.load_data()\n",
    "train_images = train_images / 255.0\n",
    "\n",
    "x_train = train_images.reshape(-1, 28, 28, 1) #add an additional dimension to represent the single-channel\n",
    "x_test = test_images.reshape(-1, 28, 28, 1)\n",
    "\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "938/938 [==============================] - 14s 9ms/step - loss: 0.2202 - accuracy: 0.9270\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 8s 9ms/step - loss: 0.0549 - accuracy: 0.9831\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 8s 8ms/step - loss: 0.0418 - accuracy: 0.9871\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 7s 8ms/step - loss: 0.0333 - accuracy: 0.9895\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 7s 8ms/step - loss: 0.0305 - accuracy: 0.9904\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 3.7645 - accuracy: 0.9901\n",
      "Test loss: 3.7645273208618164\n",
      "Test accuracy: 0.9901000261306763\n"
     ]
    }
   ],
   "source": [
    "\n",
    "LOG_DIR = f\"{int(time.time())}\"\n",
    "\n",
    "regular_model = tf.keras.models.Sequential(name='CNN_Increasing_Filters')\n",
    "\n",
    "regular_model.add(Layer.Conv2D(8, (3, 3), padding='same', activation='relu')) \n",
    "regular_model.add(Layer.Conv2D(9, (3, 3), padding='same', activation='relu'))\n",
    "regular_model.add(Layer.Conv2D(10, (3, 3),padding='same', activation='relu'))\n",
    "regular_model.add(Layer.MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n",
    "\n",
    "regular_model.add(Layer.Conv2D(20, (3, 3), padding='same', activation='relu'))\n",
    "regular_model.add(Layer.Conv2D(21, (3, 3), padding='same', activation='relu'))\n",
    "regular_model.add(Layer.Conv2D(22, (3, 3),padding='same', activation='relu'))\n",
    "regular_model.add(Layer.MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n",
    "\n",
    "regular_model.add(Layer.Conv2D(44, (3, 3), padding='same', activation='relu'))\n",
    "regular_model.add(Layer.Conv2D(45, (3, 3), padding='same', activation='relu'))\n",
    "regular_model.add(Layer.Conv2D(46, (3, 3), padding='same', activation='relu'))\n",
    "regular_model.add(Layer.MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n",
    "\n",
    "regular_model.add(Layer.Conv2D(160, (3, 3), padding='same', activation='relu'))\n",
    "\n",
    "regular_model.add(Layer.Flatten())\n",
    "regular_model.add(Layer.Dense(80))\n",
    "regular_model.add(Layer.Activation('relu'))\n",
    "regular_model.add(Layer.Dense(10))\n",
    "regular_model.add(Layer.Activation('softmax'))\n",
    "\n",
    "# keras said that .01 was optimal but .001 seems to work better\n",
    "regular_model.compile(loss='sparse_categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=.001), metrics=['accuracy'])\n",
    "\n",
    "regular_model.build(input_shape=(1,28,28,1))\n",
    "\n",
    "\n",
    "# Define the Keras TensorBoard callback.\n",
    "logdir=\"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "batch_sizes = [64, 128, 256, 512, 1024]\n",
    "# for i in range(len(1)):\n",
    "\n",
    "\n",
    "regular_model.fit(\n",
    "    x_train,\n",
    "    train_labels, \n",
    "    batch_size=64,\n",
    "    epochs=5,\n",
    "    callbacks=[tensorboard_callback])\n",
    "\n",
    "# print(f\"batch_size: {int(batch_sizes[)}\")\n",
    "score = regular_model.evaluate(x_test, test_labels)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "# Report For Regular Model\n",
    "<br>\n",
    "\n",
    "At first, I manually changed my hyperparameters and then ran my models. I later discovered the Karas-Tuner and that drastically improved the ability of my hyperparameter testing. \n",
    "I used the tuner for most of the hyperparameters, but I had to manually test the batch size with a for loop.\n",
    "\n",
    "# Summary\n",
    "\n",
    "Number of models tested: 40\n",
    "<br>\n",
    "\n",
    "Best:<br>\n",
    "- Number Filters 10th Conv. Layer: 160\n",
    "- Number Neurons Hidden Layer: 80\n",
    "- Learning Rate: .001\n",
    "- Optimizer: Adam\n",
    "- Batch Size: 64\n",
    "\n",
    "<br>\n",
    "\n",
    "- epochs: 5\n",
    "- Test loss: 7.187195301055908\n",
    "- Test accuracy: 0.9861000180244446\n",
    "\n",
    "\n",
    "## Values Tuned With Karas-Tuner\n",
    "\n",
    "- learning rate\n",
    "- optimizer\n",
    "- batch size\n",
    "- Number of filters in 10th convolution layer\n",
    "- Number of neurons in hidden layer\n",
    "\n",
    "\n",
    "--- \n",
    "\n",
    "# Optimizing Neurons in Hidden Layer & 10th Conv. Layer (Keras)\n",
    "\n",
    "First, I ran models with various combinations of the following values:\n",
    "\n",
    "## Parameters Tested\n",
    "- Filters in 10th Conv.: Range: [50 - 100], Step Size: 10\n",
    "- Neurons in Hidden Layer.: Range: [20 - 100], Step Size: 20\n",
    "\n",
    "## Number of Models Tested\n",
    "- 20\n",
    "\n",
    "## Top Three Combinations\n",
    "1. \n",
    "- conv10_filters: 100\n",
    "- hidden_neurons: 80\n",
    "- Score: 0.9649999737739563\n",
    "2. \n",
    "- conv10_filters: 80\n",
    "- hidden_neurons: 80\n",
    "- Score: 0.9643999934196472\n",
    "3. \n",
    "- conv10_filters: 80\n",
    "- hidden_neurons: 60\n",
    "- Score: 0.9632999897003174\n",
    "\n",
    "<br>\n",
    "\n",
    "Next, since I found a good value for the number of neurons in the hidden layer, I began further testing the number of filters.\n",
    "\n",
    "## Parameters Tested\n",
    "- Filters in 10th Conv.: Range: [100 - 200], Step Size: 20\n",
    "\n",
    "## Number of Models Tested\n",
    "- 6\n",
    "  \n",
    "## Top Three Combinations\n",
    "1. \n",
    "- conv10_filters: 160\n",
    "- Score: 0.9704999923706055\n",
    "2. \n",
    "- conv10_filters: 180\n",
    "- Score: 0.9703999757766724\n",
    "3. \n",
    "- conv10_filters: 100\n",
    "- Score: 0.9675999879837036\n",
    "\n",
    "Now that I've tuned the model, I can work on the hyper parameters.\n",
    "\n",
    "--- \n",
    "# Optimizing Learning Rate & Optimizer (Keras)\n",
    "\n",
    "First, I ran models with various combinations of the following values:\n",
    "\n",
    "## Parameters Tested\n",
    "- Learning Rate: Choices: [.01, .001, .0001]\n",
    "- Optimizer: Choices: [adam, SGD, RMSprop]\n",
    "\n",
    "## Number of Models Tested\n",
    "- 9\n",
    "\n",
    "## Top Three Combinations\n",
    "1. \n",
    "- optimizer: adam\n",
    "- learning_rate: 0.01\n",
    "- Score: 0.9660000205039978\n",
    "2. \n",
    "- optimizer: adam\n",
    "- learning_rate: 0.001\n",
    "- Score: 0.9620000123977661\n",
    "3. \n",
    "- optimizer: adam\n",
    "- learning_rate: 0.0001\n",
    "- Score: 0.9588000178337097\n",
    "\n",
    "As you can see, the obvious best optimizer is adam with a learning rate of .01.\n",
    "\n",
    "--- \n",
    "\n",
    "# Optimizing Batch Size (For Loop)\n",
    "\n",
    "I ran models with various batch size values:\n",
    "\n",
    "## Parameters Tested\n",
    "- Batch Size: Choices: [64, 128, 256, 512, 1024]\n",
    "\n",
    "## Number of Models Tested\n",
    "- 5\n",
    "\n",
    "## Top Three Combinations\n",
    "1. \n",
    "- batch_size: 512\n",
    "- Score: .76\n",
    "2. \n",
    "- batch_size: 64\n",
    "- Score: .11\n",
    "3. \n",
    "- batch_size: 128\n",
    "- Score: .11\n",
    "\n",
    "As you can see the best batch size for the model is 512.\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inverted Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469/469 [==============================] - 16s 31ms/step - loss: 0.2634 - accuracy: 0.9138\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 6.4808 - accuracy: 0.9824\n",
      "Test loss: 6.480801582336426\n",
      "Test accuracy: 0.9824000000953674\n"
     ]
    }
   ],
   "source": [
    "inverted_model = tf.keras.models.Sequential(name='CNN_Decreasing_Filters')\n",
    "\n",
    "inverted_model.add(Layer.Conv2D(80, (3, 3), padding='same', activation='relu')) \n",
    "inverted_model.add(Layer.Conv2D(75, (3, 3), padding='same', activation='relu'))\n",
    "inverted_model.add(Layer.Conv2D(70, (3, 3),padding='same', activation='relu'))\n",
    "inverted_model.add(Layer.MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n",
    "\n",
    "inverted_model.add(Layer.Conv2D(55, (3, 3), padding='same', activation='relu'))\n",
    "inverted_model.add(Layer.Conv2D(50, (3, 3), padding='same', activation='relu'))\n",
    "inverted_model.add(Layer.Conv2D(45, (3, 3),padding='same', activation='relu'))\n",
    "inverted_model.add(Layer.MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n",
    "\n",
    "inverted_model.add(Layer.Conv2D(25, (3, 3), padding='same', activation='relu'))\n",
    "inverted_model.add(Layer.Conv2D(20, (3, 3), padding='same', activation='relu'))\n",
    "inverted_model.add(Layer.Conv2D(15, (3, 3), padding='same', activation='relu'))\n",
    "inverted_model.add(Layer.MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n",
    "\n",
    "inverted_model.add(Layer.Conv2D(10, (3, 3), padding='same', activation='relu'))\n",
    "\n",
    "inverted_model.add(Layer.Flatten())\n",
    "inverted_model.add(Layer.Dense(100))\n",
    "inverted_model.add(Layer.Activation('relu'))\n",
    "inverted_model.add(Layer.Dense(10))\n",
    "inverted_model.add(Layer.Activation('softmax'))\n",
    "\n",
    "inverted_model.compile(loss='sparse_categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=.001), metrics=['accuracy'])\n",
    "\n",
    "inverted_model.build(input_shape=(1,28,28,1))\n",
    "\n",
    "\n",
    "# Define the Keras TensorBoard callback.\n",
    "logdir=\"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "# Train the model.\n",
    "inverted_model.fit(\n",
    "    x_train,\n",
    "    train_labels, \n",
    "    batch_size=128,\n",
    "    epochs=1,\n",
    "    callbacks=[tensorboard_callback])\n",
    "\n",
    "# Evaluate\n",
    "score = inverted_model.evaluate(x_test, test_labels)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "# Report For Inverted Model\n",
    "<br>\n",
    "\n",
    "# Summary\n",
    "\n",
    "Number of models tested: 36\n",
    "<br>\n",
    "\n",
    "Best:<br>\n",
    "- Learning Rate: .001\n",
    "- Optimizer: Adam\n",
    "- Batch Size: 128\n",
    "\n",
    "<br>\n",
    "\n",
    "- epochs: 5\n",
    "- Test loss: 5.121163845062256\n",
    "- Test accuracy: 0.9890999794006348\n",
    "\n",
    "\n",
    "At first, I manually changed my hyperparameters and then ran my models. I later discovered the Karas-Tuner and that drastically improved the ability of my hyperparameter testing. \n",
    "I used the tuner for most of the hyperparameters, but I had to manually test the batch size with a for loop.\n",
    "\n",
    "This network took far longer to train compared to the other models.\n",
    "\n",
    "## Values Tuned With Karas-Tuner\n",
    "\n",
    "- learning rate\n",
    "- optimizer\n",
    "- batch size\n",
    "\n",
    "---\n",
    "\n",
    "# Optimizing Learning Rate & Optimizer (Keras)\n",
    "\n",
    "First, I ran models with various combinations of the following values:\n",
    "\n",
    "## Parameters Tested\n",
    "- Learning Rate: Choices: [.01, .001, .0001]\n",
    "- Optimizer: Choices: [adam, SGD, RMSprop]\n",
    "\n",
    "## Number of Models Tested\n",
    "- 9\n",
    "\n",
    "## Top Five Combinations\n",
    "1. \n",
    "- optimizer: adam\n",
    "- learning_rate: 0.01\n",
    "- Score: 0.982699990272522\n",
    "2. \n",
    "- optimizer: adam\n",
    "- learning_rate: 0.001\n",
    "- Score: 0.9800000190734863\n",
    "3. \n",
    "- optimizer: adam\n",
    "- learning_rate: 0.0001\n",
    "- Score: 0.9796000123023987\n",
    "4. \n",
    "- optimizer: RMSprop\n",
    "- learning_rate: 0.01\n",
    "- Score: 0.9779999852180481\n",
    "5. \n",
    "- optimizer: RMSprop\n",
    "- learning_rate: 0.0001\n",
    "- Score: 0.9771999716758728\n",
    "\n",
    "As you can see, the obvious best optimizer is adam with a learning rate of .01. Though the test says to use .01, for some reason, the .001 learning rate seems to perform better.\n",
    "\n",
    "--- \n",
    "\n",
    "# Optimizing Batch Size (For Loop)\n",
    "\n",
    "I ran models with various batch size values:\n",
    "\n",
    "## Parameters Tested\n",
    "- Batch Size: Choices: [128, 512, 1024]\n",
    "\n",
    "## Number of Models Tested\n",
    "- 27\n",
    "\n",
    "## Top Three Combinations\n",
    "1. \n",
    "- batch_size: 128\n",
    "- Score: 0.982699990272522\n",
    "2. \n",
    "- batch_size: 512\n",
    "- Score: 0.9668999910354614\n",
    "3. \n",
    "- batch_size: 1024\n",
    "- Score: 0.9431999921798706\n",
    "\n",
    "As you can see the best batch size for the model is 128. Interestingly, the bigger batch sizes seemed to perform better with a smaller learning rate of .0001 opposed to .01 with a smaller batch size.\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hourglass Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 8s 54ms/step - loss: 0.5494 - accuracy: 0.8216\n",
      "313/313 [==============================] - 2s 4ms/step - loss: 17.3709 - accuracy: 0.9616\n",
      "Test loss: 17.370874404907227\n",
      "Test accuracy: 0.9616000056266785\n"
     ]
    }
   ],
   "source": [
    "hourglass_model = tf.keras.models.Sequential(name='CNN_HourGlass_Filters')\n",
    "\n",
    "hourglass_model.add(Layer.Conv2D(30, (3, 3), padding='same', activation='relu')) \n",
    "hourglass_model.add(Layer.Conv2D(31, (3, 3), padding='same', activation='relu'))\n",
    "hourglass_model.add(Layer.Conv2D(32, (3, 3),padding='same', activation='relu'))\n",
    "hourglass_model.add(Layer.MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n",
    "\n",
    "hourglass_model.add(Layer.Conv2D(60, (3, 3), padding='same', activation='relu'))\n",
    "hourglass_model.add(Layer.Conv2D(65, (3, 3), padding='same', activation='relu'))\n",
    "hourglass_model.add(Layer.Conv2D(60, (3, 3),padding='same', activation='relu'))\n",
    "hourglass_model.add(Layer.MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n",
    "\n",
    "hourglass_model.add(Layer.Conv2D(32, (3, 3), padding='same', activation='relu'))\n",
    "hourglass_model.add(Layer.Conv2D(31, (3, 3), padding='same', activation='relu'))\n",
    "hourglass_model.add(Layer.Conv2D(30, (3, 3), padding='same', activation='relu'))\n",
    "hourglass_model.add(Layer.MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n",
    "\n",
    "hourglass_model.add(Layer.Conv2D(25, (3, 3), padding='same', activation='relu'))\n",
    "\n",
    "hourglass_model.add(Layer.Flatten())\n",
    "hourglass_model.add(Layer.Dense(128))\n",
    "hourglass_model.add(Layer.Activation('relu'))\n",
    "hourglass_model.add(Layer.Dense(10))\n",
    "hourglass_model.add(Layer.Activation('softmax'))\n",
    "\n",
    "hourglass_model.compile(loss='sparse_categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=.001), metrics=['accuracy'])\n",
    "\n",
    "hourglass_model.build(input_shape=(1,28,28,1))\n",
    "\n",
    "# Define the Keras TensorBoard callback.\n",
    "logdir=\"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "# Train the model.\n",
    "hourglass_model.fit(\n",
    "    x_train,\n",
    "    train_labels, \n",
    "    batch_size=512,\n",
    "    epochs=1,\n",
    "    callbacks=[tensorboard_callback])\n",
    "\n",
    "# Evaluate\n",
    "score = hourglass_model.evaluate(x_test, test_labels)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "# Report For Hourglass Model\n",
    "<br>\n",
    "\n",
    "# Summary\n",
    "\n",
    "Number of models tested: 36\n",
    "<br>\n",
    "\n",
    "Best:<br>\n",
    "- Learning Rate: .001\n",
    "- Optimizer: Adam\n",
    "- Batch Size: 128\n",
    "\n",
    "<br>\n",
    "\n",
    "- epochs: 5\n",
    "- Test loss: 6.594151496887207\n",
    "- Test accuracy: 0.9869999885559082\n",
    "\n",
    "\n",
    "At first, I manually changed my hyperparameters and then ran my models. I later discovered the Karas-Tuner and that drastically improved the ability of my hyperparameter testing. \n",
    "I used the tuner for most of the hyperparameters, but I had to manually test the batch size with a for loop.\n",
    "\n",
    "This network took far longer to train compared to the other models.\n",
    "\n",
    "\n",
    "## Values Tuned With Karas-Tuner\n",
    "\n",
    "- learning rate\n",
    "- optimizer\n",
    "- batch size\n",
    "\n",
    "---\n",
    "\n",
    "# Optimizing Learning Rate & Optimizer (Keras)\n",
    "\n",
    "First, I ran models with various combinations of the following values:\n",
    "\n",
    "## Parameters Tested\n",
    "- Learning Rate: Choices: [.01, .001, .0001]\n",
    "- Optimizer: Choices: [adam, SGD, RMSprop]\n",
    "\n",
    "## Number of Models Tested\n",
    "- 9\n",
    "\n",
    "## Top Five Combinations\n",
    "1. \n",
    "- optimizer: adam\n",
    "- learning_rate: 0.001\n",
    "- Score: 0.9689000248908997\n",
    "2. \n",
    "- optimizer: adam\n",
    "- learning_rate: 0.01\n",
    "- Score: 0.965399980545044\n",
    "3. \n",
    "- optimizer: adam\n",
    "- learning_rate: 0.0001\n",
    "- Score: 0.961899995803833\n",
    "4. \n",
    "- optimizer: RMSprop\n",
    "- learning_rate: 0.0001\n",
    "- Score: 0.9186999797821045\n",
    "5. \n",
    "- optimizer: RMSprop\n",
    "- learning_rate: 0.01\n",
    "- Score: 0.8939999938011169\n",
    "\n",
    "As you can see, the obvious best optimizer is adam with a learning rate of .001. This is the only model that actually had .001 as a better learning rate than .01.\n",
    "\n",
    "---\n",
    "\n",
    "# Optimizing Batch Size (For Loop)\n",
    "\n",
    "I ran models with various batch size values:\n",
    "\n",
    "## Parameters Tested\n",
    "- Batch Size: Choices: [128, 512, 1024]\n",
    "\n",
    "## Number of Models Tested\n",
    "- 27\n",
    "\n",
    "## Top Three Combinations\n",
    "1. \n",
    "- batch_size: 128\n",
    "- Score: 0.984499990940094\n",
    "2. \n",
    "- batch_size: 512\n",
    "- Score: 0.9689000248908997\n",
    "3. \n",
    "- batch_size: 1024\n",
    "- Score: 0.943799972534179\n",
    "\n",
    "As you can see the best batch size for the model is 128. Interestingly, the bigger batch sizes seemed to perform better with a smaller learning rate of .0001 opposed to .01 with a smaller batch size.\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "# Final Summary\n",
    "\n",
    "- Each model trained to a 98% validation accuracy after the 5th epoch. This suggests that for this problem, the number of filters and/or hidden neurons isn't as important when training the model.\n",
    "- The main differences between the models was that some of the models trained faster than others. The hourglass model seemed to train the fastest for me, but that may also be attributed to the number of trainable parameters. \n",
    "- Despite the keras-tuner tests, the best learning rate for each model proved to be .001. \n",
    "- Each model also favored the adam optimizer and RMSprop as a close second. These two optimizers greatly outperformed the stochastic gradient descent optimizer. \n",
    "- Lastly, each model seemed to achieve a higher validation accuracy when the batch size was smaller; though, it's important to keep in mind that smaller batch sizes drastically increases training times. I think this is because smaller batches means more input sets and it gives the network more opportunities to perform backprop.\n",
    "- I think the training favored the regular and hourglass models because the network doesn't perform as well with initially having larger number of filters."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
