{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension.\n",
    "%load_ext tensorboard\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.layers as Layer\n",
    "\n",
    "import tensorboard\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset and normalize\n",
    "(train_images, train_labels), (test_images, test_labels) = keras.datasets.cifar10.load_data()\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "x_train = train_images.reshape(-1, 32, 32, 3) #add an additional dimension to represent the single-channel\n",
    "x_test = test_images.reshape(-1, 32, 32, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize data by plotting images\n",
    "fig, ax = plt.subplots(5, 5)\n",
    "k = 0\n",
    " \n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        ax[i][j].imshow(x_train[k], aspect='auto')\n",
    "        k += 1\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional LeNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_net_model = tf.keras.models.Sequential(name='CNN_LeNet')\n",
    "\n",
    "le_net_model.add(Layer.Conv2D(6, (5, 5), padding='same', activation='relu'))\n",
    "le_net_model.add(Layer.MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n",
    "\n",
    "le_net_model.add(Layer.Conv2D(16, (5, 5), padding='same', activation='relu'))\n",
    "le_net_model.add(Layer.MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n",
    "\n",
    "le_net_model.add(Layer.Conv2D(120, (5, 5), padding='same', activation='relu'))\n",
    "\n",
    "le_net_model.add(Layer.Flatten())\n",
    "le_net_model.add(Layer.Dense(84))\n",
    "le_net_model.add(Layer.Activation('relu'))\n",
    "\n",
    "le_net_model.add(Layer.Dense(10))\n",
    "le_net_model.add(Layer.Activation('softmax'))\n",
    "\n",
    "le_net_model.compile(loss='sparse_categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=.001), metrics=['accuracy'])\n",
    "\n",
    "le_net_model.build(input_shape=(1,32,32,3))\n",
    "\n",
    "le_net_model.summary()\n",
    "\n",
    "# Define the Keras TensorBoard callback.\n",
    "logdir=\"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='accuracy', min_delta=.0001, patience=3, verbose=5,\n",
    "    mode='auto', baseline=None, restore_best_weights=False\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Train the model.\n",
    "le_net_model.fit(\n",
    "    x_train,\n",
    "    train_labels, \n",
    "    batch_size=512,\n",
    "    epochs=25,\n",
    "    callbacks=[tensorboard_callback, early_stopping_callback])\n",
    "\n",
    "# Evaluate\n",
    "score = le_net_model.evaluate(x_test, test_labels)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report For Convolutional LeNet Model\n",
    "<br>\n",
    "\n",
    "## Question 1\n",
    "\n",
    "The learning rate, though important, didn't seem to have as big as an effect when using the Adam optimizer. For example, the validation accuracies for .001, .0001, and .01 learning rates were respectively .963, .961, and .958. This shows that there was only a .005 accuracy difference from the lowest to the highest accuracies. This indicates that at least when using the Adam optimizer on this specific problem, the learning rate doesn't have a huge impact on the validation accuracy. The best overall learning rate proved to be .001.\n",
    "\n",
    "\n",
    "## Question 2\n",
    "\n",
    "The batch size does have a bigger impact on the validation accuracy than the learning rate did. I tested 4 different batch sizes ([64, 128, 512, 1024]) and there was a much larger .045 difference from the highest validation accuracy to the lowest. The best batch size from the set proved to be 512 with about .68 validation accuracy.\n",
    "\n",
    "\n",
    "## Question 3\n",
    "\n",
    "I used the Karas-Tuner to optimize several of my hyperparamters. I was able to optimize the learning rate and optimizer with Karas-Tuner and I optimized the batch size by manually changing it and rerunning the model. The best hyperparameters are as follows:\n",
    "- Learning Rate: .001\n",
    "- Optimizer: Adam\n",
    "- Batch Size: 512\n",
    "- Epochs: 41\n",
    "\n",
    "As you can see, the model trained to a .9901 accuracy on the training data. After 41 epochs, the model began to lose validation accuracy.\n",
    "\n",
    "The best scores I was able to achieve on this model were:\n",
    "\n",
    "25 epochs:\n",
    "\n",
    "- Test loss: 1.060050368309021\n",
    "- Test accuracy: 0.6682000160217285\n",
    "\n",
    "41 epochs\n",
    "- Test loss: 1.077089786529541\n",
    "- Test accuracy: 0.6805999875068665     \n",
    "\n",
    "![Accuracy vs Epoch](accuracy_vs_epoch.png \"Accuracy vs Epoch\")\n",
    "\n",
    "\n",
    "## Question 4\n",
    "\n",
    "### Part A\n",
    "After running the equivalent feed forward network with the Adam optimizer and a .001 learning for 25 epochs, I got the following results:\n",
    "\n",
    "Test loss: 1.738857626914978\n",
    "Test accuracy: 0.36800000071525574\n",
    "\n",
    "As you can see, testing the trained network led to a 36.8% validation accuracy. This score is far worse than the equivalent convolutional network's accuracy of 66.8%.\n",
    "\n",
    "\n",
    "--- \n",
    "\n",
    "\n",
    "### Part B\n",
    "There are 31,604 parameters in the normal feed forward network compared to the 697,046 parameters in the convolutional LeNet model. Is the greater number of parameters worth it? The convolutional model has 22.06 times more parameters than feed forward model and achieves a 84.24% increase in model accuracy. I think this increase in parameters is worth the increase in validation accuracy; especially since the training time still didn't take that long.\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed Forward LeNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential Model\n",
    "feed_forward_model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(32, 32, 3)),\n",
    "    keras.layers.Dense(6, activation='relu'),\n",
    "    keras.layers.Dense(16, activation='relu'),\n",
    "    keras.layers.Dense(120, activation='relu'),\n",
    "    keras.layers.Dense(84, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "feed_forward_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "feed_forward_model.summary()\n",
    "\n",
    "\n",
    "# Define the Keras TensorBoard callback.\n",
    "logdir=\"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "# Train the model.\n",
    "feed_forward_model.fit(\n",
    "    x_train,\n",
    "    train_labels, \n",
    "    batch_size=512,\n",
    "    epochs=25,\n",
    "    callbacks=[tensorboard_callback])\n",
    "\n",
    "# Evaluate\n",
    "score = feed_forward_model.evaluate(x_test, test_labels)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7 5 0 0 3 2]\n",
      " [6 4 5 1 4 8]\n",
      " [9 0 2 2 5 4]\n",
      " [6 3 4 7 9 8]\n",
      " [5 7 5 6 9 0]\n",
      " [7 9 0 8 2 3]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[7,5,0,0,3,2], [6,4,5,1,4,8], [9,0,2,2,5,4], [6,3,4,7,9,8], [5,7,5,6,9,0], [7,9,0,8,2,3]])\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"CNN_LeNet\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 1, 6, 6, 1)        10        \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 36)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 84)                3108      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 84)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                850       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 3,968\n",
      "Trainable params: 3,968\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "le_net_model = tf.keras.models.Sequential(name='CNN_LeNet')\n",
    "\n",
    "le_net_model.add(Layer.Conv2D(1, (3, 3), padding='same', activation='relu', input_shape=(1,6,6,1)))\n",
    "# le_net_model.add(Layer.MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n",
    "\n",
    "\n",
    "le_net_model.add(Layer.Flatten())\n",
    "le_net_model.add(Layer.Dense(84))\n",
    "le_net_model.add(Layer.Activation('relu'))\n",
    "\n",
    "le_net_model.add(Layer.Dense(10))\n",
    "le_net_model.add(Layer.Activation('softmax'))\n",
    "\n",
    "le_net_model.compile(loss='sparse_categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=.001), metrics=['accuracy'])\n",
    "\n",
    "le_net_model.build()\n",
    "\n",
    "le_net_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Train the model.\n",
    "le_net_model.fit(\n",
    "    x_train,\n",
    "    train_labels, \n",
    "    batch_size=512,\n",
    "    epochs=25,\n",
    "    callbacks=[tensorboard_callback, early_stopping_callback])\n",
    "\n",
    "# Evaluate\n",
    "score = le_net_model.evaluate(x_test, test_labels)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "11f1dc213e07634baa4c5c321dec03c05dafae643c50f20e6d1a492290c05dc2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
